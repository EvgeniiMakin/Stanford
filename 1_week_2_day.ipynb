{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Representation  \n",
    "To establish notation for future use, we’ll use $x^{(i)}$ \n",
    "  to denote the “input” variables (living area in this example), also called input features, and $y^{(i)}$\n",
    "  to denote the “output” or target variable that we are trying to predict (price). A pair $(x^{(i)} , y^{(i)} )$ is called a training example, and the dataset that we’ll be using to learn—a list of m training examples $(x ^ {(i)}, y ^ {(i)});i=1,...,m$ — is called a training set. Note that the superscript $“(i)”$ in the notation is simply an index into the training set, and has nothing to do with exponentiation. We will also use X to denote the space of input values, and Y to denote the space of output values. In this example, X = Y = ℝ.\n",
    "\n",
    "To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. Seen pictorially, the process is therefore like this:\n",
    "![Image of Yaktocat](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/H6qTdZmYEeaagxL7xdFKxA_2f0f671110e8f7446bb2b5b2f75a8874_Screenshot-2016-10-23-20.14.58.png?expiry=1531958400000&hmac=Au-xxwBMAfz06ZF-4ApfnAKNInIrQa0ocKXi28RRIuE)\n",
    "\n",
    "When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem. When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem.\n",
    "\n",
    "\n",
    "Представление модели  \n",
    "Чтобы установить нотацию для будущего использования, мы будем использовать $ x ^ {(i)} $\n",
    "  для обозначения «входных» переменных (жилая площадь в этом примере), также называемых входными функциями, и $ y ^ {(i)} $\n",
    "  для обозначения «выходной» или целевой переменной, которую мы пытаемся предсказать (цена). Пара $ (x ^ {(i)}, y ^ {(i)}) $ называется примером обучения и набором данных, который мы будем изучать, - список примеров обучения m $ (x ^ {(i)}, y ^ {(i)}); i = 1, ..., m $ - называется обучающим множеством. Обратите внимание, что верхний индекс $ \"(i)\" $ в обозначении является просто индексом в обучающем наборе и не имеет ничего общего с возведением в степень. Мы также будем использовать X для обозначения пространства входных значений, а Y - пространство выходных значений. В этом примере X = Y = ℝ.\n",
    "\n",
    "Чтобы более подробно описать контролируемую проблему обучения, наша цель состоит в том, чтобы с помощью учебного набора изучить функцию h: X → Y, так что h (x) является «хорошим» предсказателем для соответствующего значения y. По историческим причинам эта функция h называется гипотезой. Видно, что процесс выглядит следующим образом:\n",
    "![Image of Yaktocat](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/H6qTdZmYEeaagxL7xdFKxA_2f0f671110e8f7446bb2b5b2f75a8874_Screenshot-2016-10-23-20.14.58.png?expiry=1531958400000&hmac=Au-xxwBMAfz06ZF-4ApfnAKNInIrQa0ocKXi28RRIuE)\n",
    "\n",
    "Когда целевая переменная, которую мы пытаемся предсказать, непрерывна, например, в нашем примере корпуса, мы называем проблему обучения проблемой регрессии. Когда y может принимать только небольшое количество дискретных значений (например, если бы, учитывая жилую площадь, мы хотели предсказать, является ли жильем дом или квартира, скажем), мы называем это проблемой классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Function  \n",
    "We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n",
    "\n",
    "$J(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left ( \\hat{y}_{i}- y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2$\n",
    "\n",
    "To break it apart, it is $\\frac{1}{2} \\bar{x} $\n",
    "where $\\bar{x}$ is the mean of the squares of $h_\\theta (x_{i}) - y_{i}$, or the difference between the predicted value and the actual value.\n",
    "\n",
    "This function is otherwise called the **\"Squared error function\", or \"Mean squared error\"**. The mean is halved $\\left(\\frac{1}{2}\\right)$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2}$ term. The following image summarizes what the cost function does:\n",
    "![Image of Yaktocat](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/R2YF5Lj3EeajLxLfjQiSjg_110c901f58043f995a35b31431935290_Screen-Shot-2016-12-02-at-5.23.31-PM.png?expiry=1531958400000&hmac=3ESPB6RQ2DwQGCaIdAQPXaMvRLvPM87IZ4cuCPFV-54)\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "Функция стоимости  \n",
    "Мы можем измерить точность нашей функции гипотез, используя функцию стоимости. Это имеет среднюю разницу (фактически более благоприятную версию среднего) всех результатов гипотезы с входами от x и фактического вывода y.\n",
    "\n",
    "$J(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left ( \\hat{y}_{i}- y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2$\n",
    "\n",
    "Чтобы разбить его,$\\frac{1}{2} \\bar{x} $\n",
    "где $\\bar{x}$  - среднее квадратов $h_\\theta (x_{i}) - y_{i}$ или разность между прогнозируемым значением и фактическим значением.\n",
    "\n",
    "Эту функцию иначе называют «функцией ошибки квадрата» или «среднеквадратичной ошибкой». Среднее значение уменьшено $\\left(\\frac{1}{2}\\right)$ в качестве удобства для вычисления градиентного спуска, так как производный член квадратной функции сократит член $\\frac{1}{2}$. Следующее изображение суммирует, что делает функция стоимости:\n",
    "![Image of Yaktocat](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/R2YF5Lj3EeajLxLfjQiSjg_110c901f58043f995a35b31431935290_Screen-Shot-2016-12-02-at-5.23.31-PM.png?expiry=1531958400000&hmac=3ESPB6RQ2DwQGCaIdAQPXaMvRLvPM87IZ4cuCPFV-54)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function - Intuition I   \n",
    "https://www.coursera.org/learn/machine-learning/supplement/u3qF5/cost-function-intuition-i\n",
    "## Cost Function - Intuition II\n",
    "https://www.coursera.org/learn/machine-learning/supplement/9SEeJ/cost-function-intuition-ii\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "https://www.coursera.org/learn/machine-learning/supplement/2GnUg/gradient-descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Intuition\n",
    "https://www.coursera.org/learn/machine-learning/supplement/QKEdR/gradient-descent-intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent For Linear Regression\n",
    "https://www.coursera.org/learn/machine-learning/supplement/U90DX/gradient-descent-for-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Matrices and Vectors\n",
    "https://www.coursera.org/learn/machine-learning/supplement/Q6mSN/matrices-and-vectors\n",
    "* ### Addition and Scalar Multiplication\n",
    "https://www.coursera.org/learn/machine-learning/supplement/FenyC/addition-and-scalar-multiplication\n",
    "* ### Matrix-Vector Multiplication\n",
    "https://www.coursera.org/learn/machine-learning/supplement/cgVgM/matrix-vector-multiplication\n",
    "* ### Matrix-Matrix Multiplication\n",
    "https://www.coursera.org/learn/machine-learning/supplement/l0myT/matrix-matrix-multiplication\n",
    "* ### Matrix Multiplication Properties\n",
    "https://www.coursera.org/learn/machine-learning/supplement/Xl0xT/matrix-multiplication-properties\n",
    "* ### Inverse and Transpose\n",
    "https://www.coursera.org/learn/machine-learning/supplement/EcNto/inverse-and-transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

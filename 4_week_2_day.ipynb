{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Model Representation I\n",
    "https://www.coursera.org/learn/machine-learning/supplement/Bln5m/model-representation-i\n",
    "\n",
    "#### Our $x_{0}$ input node is sometimes called the \"bias unit.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0:00\n",
    "В этом видео я хочу рассказать вам о том, как мы представляем нейронные сети. Другими словами, как мы представляем нашу гипотезу или как мы представляем нашу модель при использовании нейронных сетей. Нейронные сети были разработаны как имитирующие нейроны или сети нейронов в головном мозге. Итак, чтобы объяснить представление гипотезы, давайте начнем с изучения того, как выглядит один нейрон в мозге. Ваш мозг и мое - это джем, заполненный нейронами, подобными этим, и нейроны - это клетки в мозге. И две вещи, на которые нужно обратить внимание, - это в первую очередь. Нейрон имеет тело клетки, например, и, кроме того, нейрон имеет несколько входных проводов, и они называются дендритами. Вы считаете их входными проводами, и они получают входные данные из других мест. И у нейрона также есть выходной провод под названием Axon, и этот выходной провод - это то, что он использует для отправки сигналов другим нейронам, поэтому отправлять сообщения другим нейронам. Таким образом, на упрощенном уровне, что такое нейрон, является вычислительной единицей, которая получает несколько входных данных через входные провода и выполняет некоторые вычисления, а затем говорит о выводе через его аксон к другим узлам или к другим нейронам в мозге.\n",
    "\n",
    "Вот иллюстрация группы нейронов. То, как нейроны общаются друг с другом, - это небольшие импульсы электричества, их также называют шипами, но это означает импульсы электричества. Итак, вот один нейрон, и что он делает, если он хочет отправить сообщение, что он делает, это посылает небольшой импульс электричества. Varis axon в какой-то другой нейрон, и здесь этот аксон, который является этим открытым проводом, соединяется с дендритами этого второго нейрона здесь, который затем принимает это входящее сообщение, что некоторые вычисления. И они, в свою очередь, решили отправить это сообщение на этот аксон другим нейронам, и это процесс, посредством которого происходит всякая человеческая мысль. Именно эти нейроны выполняют вычисления и передают сообщения другим нейронам в результате того, какие другие данные у них есть. И, кстати, так работают наши чувства и наши мышцы. Если вы хотите переместить одну из ваших мышц так, как в другом месте вашего нейрона может послать это электричество в вашу мышцу, и это заставляет ваши мышцы сжиматься и ваши глаза, некоторые чувства, подобные вашему глазу, должны посылать сообщение вашему мозгу, пока оно он чувствует, что хозяин электросети к нейрону в вашем мозгу подобен этому. В нейро-сети, точнее, в искусственной нейронной сети, которую мы реализовали на компьютере, мы собираемся использовать очень простую модель того, что нейрон делает, мы собираемся моделировать нейрон как просто логистический блок , Поэтому, когда я рисую желтый круг, вы должны думать об этом как о роли анализа роли, который, возможно, является телом нейрона, а затем мы питаем нейрон несколькими входами, которые являются различными дендритами или входными ворами.\n",
    "3:14\n",
    "И нейрон делает некоторые вычисления. И вывести некоторое значение на этот выходной провод, или в биологический нейрон, это аксон. И всякий раз, когда я рисую диаграмму подобным образом, это означает, что это представляет вычисление h из x, равное единице над одним плюс e, к отрицательной тета-транспозиции x, где, как обычно, x и theta являются нашими векторами параметров, например.\n",
    "3:42\n",
    "Таким образом, это очень простая, может быть, упрощенная модель вычислений, которые выполняет нейрон, где он получает несколько входов, x1, x2, x3 и выводит некоторое значение, вычисленное так.\n",
    "3:59\n",
    "Когда я рисую нейронную сеть, я обычно рисую только входные узлы x1, x2, x3. Иногда, когда это полезно, я нарисую дополнительный узел для x0.\n",
    "4:11\n",
    "Этот x0 теперь иногда называется единицей смещения или нейронной системы смещения, но поскольку x0 уже равен 1, иногда я рисую это, иногда я не буду просто в зависимости от того, что более удобно для этого примера.\n",
    "4:31\n",
    "Наконец, последний раз терминология, когда мы говорим о нейронных сетях, иногда мы скажем, что это нейрон или искусственный нейрон с сигмоидной или логистической функцией активации. Итак, эта функция активации в терминологии нейронной сети. Это просто еще один термин для этой функции для этой нелинейности g (z) = 1 над 1 + e до -z. И пока я называл тета параметрами модели, я в основном продолжу использовать эту терминологию. Здесь это копия параметров, но в нейронных сетях в литературе нейронной сети иногда вы можете услышать, как люди говорят о весах модели, а весы - это точно то же, что и параметры модели. Но в основном я буду использовать терминологические параметры в этих видео, но иногда вы можете услышать, что другие используют терминологию весов.\n",
    "5:27\n",
    "Итак, эта маленькая диаграмма представляет собой один нейрон.\n",
    "5:34\n",
    "Что такое нейронная сеть, это всего лишь группа этих разных нейронов. Полностью, здесь у нас есть единицы ввода x1, x2, x3 и еще раз, иногда вы можете нарисовать эту дополнительную заметку x0, а иногда нет, просто поместите это здесь. И здесь у нас есть три нейроны, которые написали 81, 82, 83. Я расскажу об этих показателях позже. И еще раз мы сможем, если хотим добавить только a0 и добавить там блок смещения смеси. Всегда есть значение 1. И наконец, у нас есть этот третий узел и последний уровень, и есть третий узел, который выводит значение, которое вычисляет гипотеза h (x). Чтобы ввести немного более терминологию, в нейронной сети, в первом слое, это также называется входным слоем, потому что здесь мы вводим наши функции, x1, x2, x3. Последний слой также называется выходным слоем, потому что этот слой имеет нейрон, этот здесь, который выводит окончательное значение, вычисленное по гипотезе. И затем, слой 2 между ними, это называется скрытым слоем. Термин скрытый слой не является отличной терминологией, но это соображение состоит в том, что вы знаете, что вы контролировали на раннем этапе, где вы можете увидеть входные данные и увидеть правильные выходы, где есть скрытый уровень значений, которые вы не используете наблюдать за тренировкой. Это не х, и это не у, и поэтому мы называем их скрытыми. И они пытаются увидеть нейронные сети с более чем одним скрытым слоем, но в этом примере у нас есть один уровень ввода: слой 1, один скрытый слой, слой 2 и один выходной слой, уровень 3. Но в основном ничего, что не входной уровень и не является выходным слоем, называется скрытым уровнем.\n",
    "7:29\n",
    "Поэтому я хочу быть в курсе того, что делает эта нейронная сеть. Давайте пройдем через вычислительные шаги, которые есть и тело, представленное этой диаграммой. Чтобы объяснить эти конкретные вычисления, представленные нейронной сетью, здесь немного больше обозначений. Я буду использовать индекс i индекса sup, чтобы обозначить активацию нейрона i или единицы i в слое j. Таким образом, полностью это дало верхний индекс подгруппе один, это активация первого блока во втором слое, в нашем скрытом слое. И при активации я имею в виду значение, которое вычисляется и выводится конкретным. Кроме того, новая сеть параметризуется этими матрицами, theta super script j где theta j будет представлять собой матрицу весов, управляющую отображением функции из одного слоя, возможно, первого слоя ко второму слою или от второго уровня к третьего слоя.\n",
    "8:30\n",
    "Итак, вот вычисления, представленные этой диаграммой.\n",
    "8:34\n",
    "Этот первый скрытый блок здесь имеет значение, рассчитанное следующим образом: a21 равно сигма-функции функции активации сигмы, также называемой функцией активации логистики, применительно к такой линейной комбинации этих входов. И тогда этот второй скрытый блок имеет этот компьютер с активацией, как сигмоид этого. И аналогичным образом для этой третьей скрытой единицы вычисляется эта формула. Итак, здесь мы имеем 3 тета 1, которая является матрицей параметров, определяющих наше отображение из наших трех разных единиц, наших скрытых единиц. Тета 1 будет 3.\n",
    "9:35\n",
    "Theta 1 будет 3x4-мерной матрицей. И в более общем случае, если в сети есть единицы SJU, то j и sj + 1 единиц и sj + 1, то матрица theta j, которая управляет функцией, отображающей из нее sj + 1. Это должно будет упомянуть sj +1 посредством sj + 1 Я просто буду четко про эту нотацию. Это подстрочный индекс j + 1, и это s индекс j, а затем все это, плюс 1, все это (sj + 1), хорошо? Итак, это s индекс j + 1,\n",
    "10:21\n",
    "Итак, это s индекс j + 1 по sj + 1, где этот плюс один не является частью индекса. Хорошо, поэтому мы говорили о том, что делают три скрытых подразделения, чтобы вычислить их ценности. Наконец, есть потеря этого финала, и после этого у нас есть еще одна единица, компьютер h из x, и это равно, также может быть записано как (3) 1, и это равно этому. И вы замечаете, что я написал это с помощью надстрочного текста здесь, потому что тета надстрочного двоичного кода - это матрица параметров или матрица весов, которая управляет функцией, которая отображает из скрытых единиц, то есть единицы уровня два один слой три единицы, то есть блок вывода. Подводя итог, мы продемонстрировали, как картина, подобная этой, здесь определяет искусственную нейронную сеть, которая определяет функцию h, которая, как мы надеемся, отображает с входными значениями x в какое-то пространство, которое содержит положения y. И эти гипотезы параметризуются параметрами, обозначающими капитальную тета, так что, изменяя тета, мы получаем различную гипотезу и получаем разные функции. Картирование говорит от x к y.\n",
    "11:42\n",
    "Таким образом, это дает нам математическое определение того, как представить гипотезу в нейронной сети. В следующих нескольких видео, что я хотел бы сделать, это дать вам больше интуиции о том, что делают эти представления гипотез, а также пройти несколько примеров и рассказать о том, как их эффективно вычислить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Model Representation II\n",
    "https://www.coursera.org/learn/machine-learning/supplement/YlEVx/model-representation-ii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0:00\n",
    "В последнем видео мы дали математическое определение того, как представить или как вычислить гипотезы, используемые нейронной сетью.\n",
    "0:08\n",
    "В этом видео я хотел бы показать вам, как эффективно выполнять эти вычисления, и это покажет вам реализацию векторного роста.\n",
    "0:17\n",
    "И, во-вторых, и что более важно, я хочу начать рассказывать вам, почему эти представления нейронной сети могут быть хорошей идеей и как они могут помочь нам изучить сложные нелинейные гипотезы.\n",
    "0:28\n",
    "Рассмотрим эту нейронную сеть. Ранее мы говорили, что последовательность шагов, которые нам нужны для вычисления вывода гипотез, - это эти уравнения, приведенные слева, где мы вычисляем значения активации трех скрытых применений, а затем мы используем их для вычисления конечного результата нашей гипотезы h х. Теперь я собираюсь определить несколько дополнительных условий. Итак, этот термин, который я подчеркиваю здесь, я собираюсь определить, что это индекс superscript 2 z 1. Итак, мы имеем, что a (2) 1, который является этим членом, равен g от z до 1. И, к слову, этот надстрочный указатель 2, вы знаете, что это означает, что z2 и этот a2 также, верхний индекс 2 в круглых скобках означает, что это значения, связанные со слоем 2, то есть со скрытым уровнем в нейронной сети ,\n",
    "1:22\n",
    "Теперь этот термин здесь я буду аналогичным образом определять как\n",
    "1:29\n",
    "г (2) 2. И, наконец, этот последний термин, который я подчеркиваю,\n",
    "1:34\n",
    "позвольте мне определить, что при z (2) 3. Итак, аналогично имеем (2) 3 равно g\n",
    "1:44\n",
    "г (2) 3. Таким образом, эти значения z представляют собой линейную комбинацию, взвешенную линейную комбинацию, входных значений x0, x1, x2, x3, которые входят в конкретный нейрон.\n",
    "1:57\n",
    "Теперь, если вы посмотрите на этот блок чисел,\n",
    "2:01\n",
    "вы можете заметить, что этот блок чисел соответствует подозрительно подобным\n",
    "2:06\n",
    "к матричной векторной операции, умножение матричного вектора x1 на вектор x. Используя это наблюдение, мы сможем векторизовать это вычисление нейронной сети.\n",
    "2:21\n",
    "Конкретно, давайте определим вектор признаков x, как обычно, как вектор x0, x1, x2, x3, где x0, как обычно, всегда равен 1 и определяет z2 как вектор этих z-значений, вы знаете, of z ( 2) 1 z (2) 2, z (2) 3.\n",
    "2:38\n",
    "И заметим, что там z2 это трехмерный вектор.\n",
    "2:43\n",
    "Теперь мы можем векторизовать вычисление\n",
    "2:48\n",
    "(2) 1, a (2) 2, a (2) 3 следующим образом. Мы можем просто написать это в два этапа. Мы можем вычислить z2 как tta 1 раз x и это даст нам этот вектор z2; а затем a2 является g из z2 и здесь просто является z2. Это трехмерный вектор, a2 также является трехмерным вектором и, следовательно, такой активацией g. Это применимо к элементу sigmoid по каждому элементу z2. И, кстати, сделать наши обозначения немного более согласованными с тем, что мы будем делать позже, на этом входном слое мы имеем входы x, но мы также можем сказать, что это так же, как при активациях первых слоев. Итак, если я определил a1 равным x. Таким образом, a1 является вектором, теперь я могу взять этот x здесь и заменить его на z2 равным theta1 раз a1, просто определяя a1 как активацию в моем входном слое.\n",
    "3:44\n",
    "Теперь, с того, что я написал до сих пор, я теперь получил значения для a1, a2, a3, и действительно, я должен также разместить над ними верхние индексы. Но мне нужно еще одно значение, и я тоже хочу это (0) 2, и это соответствует блоку смещения в скрытом слое, который идет на выход. Разумеется, здесь тоже существовала система предвзятости, которая, как вы знаете, просто не нарисовалась здесь, но чтобы позаботиться об этом дополнительном подразделении смещения, нам нужно добавить дополнительный a0 надстрочный указатель 2, равный к одному, и после этого шага мы теперь имеем, что a2 будет четырехмерным вектором признаков, потому что мы просто добавили этот дополнительный, вы знаете, a0, который равен 1, соответствующему блоку смещения в скрытом слое. И наконец,\n",
    "4:35\n",
    "для вычисления фактического значения выходных данных наших гипотез, нам просто нужно вычислить\n",
    "4:42\n",
    "z3. Таким образом, z3 здесь равен этому термину, который я только подчеркиваю. Этот внутренний член есть z3.\n",
    "4:53\n",
    "И z3 указывается 2 раза a2, и, наконец, мои гипотезы выводят h из x, который является a3, который является активацией моей единственной единицы в выходном слое. Итак, это просто реальное число. Вы можете записать его как a3 или как (3) 1, а g - z3. Этот процесс вычисления h of x также называется прямым распространением\n",
    "5:19\n",
    "и называется тем, что, поскольку мы начинаем с активизации входных единиц, а затем мы сортируем вперед-распространяем это на скрытый уровень и вычисляем активацию скрытого слоя, а затем мы вроде как прямо распространяем это и вычисляем активацию\n",
    "5:37\n",
    "выходной уровень, но этот процесс вычисления активизации с входа, затем скрытый, а затем выходной слой, и это также называется прямым распространением\n",
    "5:43\n",
    "и то, что мы только что сделали, - это просто разработка векторной мудрой реализации этой процедуры. Итак, если вы реализуете это с помощью этих уравнений, которые мы имеем справа, это даст вам эффективный способ или оба из эффективного способа вычисления h из x.\n",
    "5:58\n",
    "Этот прямой вид распространения также\n",
    "6:00\n",
    "помогает понять, что могут делать нейронные сети, и почему они могут помочь нам изучить интересные нелинейные гипотезы.\n",
    "6:08\n",
    "Рассмотрим следующую нейронную сеть и предположим, что я сейчас накрываю левый путь этой картинки. Если вы посмотрите на то, что осталось на этой картинке. Это очень похоже на логистическую регрессию, в которой мы используем эту заметку, это всего лишь единица логистической регрессии, и мы используем ее для предсказания h из x. И конкретно, то, что выводятся гипотезы, является h of x, которое будет равно g, которое является моей сигмоидной функцией активации times theta 0 раз a0, равной 1 плюс theta 1\n",
    "6:45\n",
    "плюс theta 2 раза a2 плюс theta 3 раза a3, являются ли значения a1, a2, a3 значениями, заданными этими тремя данными единицами.\n",
    "7:01\n",
    "Теперь, чтобы соответствовать моим ранним обозначениям. На самом деле, мы должны, вы знаете, заполнить эти верхние индексы 2 здесь повсюду\n",
    "7:12\n",
    "и у меня также есть эти индексы 1, потому что у меня есть только одна единица вывода, но если вы сосредоточитесь на синих частях обозначений. Это, как вы знаете, выглядит ужасно, как стандартная модель логистической регрессии, за исключением того, что у меня теперь есть тэта капитала, а не теневая.\n",
    "7:29\n",
    "И это то, что это просто логистическая регрессия.\n",
    "7:33\n",
    "Но там, где функции, подаваемые в логистическую регрессию, являются значениями, вычисленными скрытым слоем.\n",
    "7:41\n",
    "Просто повторить, что делает эта нейронная сеть, точно так же, как логистическая регрессия, за исключением того, что вместо использования оригинальных функций x1, x2, x3,\n",
    "7:52\n",
    "использует эти новые функции a1, a2, a3. Опять же, мы добавим верхние индексы\n",
    "7:58\n",
    "вы знаете, чтобы соответствовать нотации.\n",
    "8:02\n",
    "И это круто, так это то, что функции a1, a2, a3 сами изучаются как функции ввода.\n",
    "8:10\n",
    "Конкретно, отображение функции из уровня 1 в уровень 2, которое определяется некоторым другим набором параметров, theta 1. Таким образом, это как если бы нейронная сеть, вместо того, чтобы ограничить логические регрессии объектов x1, x2, x3. Он получает возможность изучать свои собственные функции a1, a2, a3 для подачи логистической регрессии и, как вы можете себе представить, в зависимости от того, какие параметры он выбирает для theta 1. Вы можете изучить некоторые интересные и сложные функции, и поэтому\n",
    "8:43\n",
    "вы можете получить лучшие гипотезы, чем если бы вы были вынуждены использовать необработанные функции x1, x2 или x3, или если вы будете сдерживать, чтобы сказать, полиномиальные термины, вы знаете, x1, x2, x3 и т. д. Но вместо этого этот алгоритм имеет гибкость, чтобы попытаться изучить все функции сразу, используя эти a1, a2, a3, чтобы прокормить эту последнюю единицу, что по существу\n",
    "9:09\n",
    "логистическая регрессия здесь. Я понял, что этот пример описан как несколько высокий уровень, и поэтому я не уверен, что эта интуиция нейронной сети, как вы знаете, имеет более сложные функции, будет иметь смысл, но если она еще не будет в следующих двух видео. Я расскажу о конкретном примере того, как нейронная сеть может использовать это скрытое там, чтобы вычислить более сложные функции для подачи на этот конечный уровень вывода и как это может изучить более сложные гипотезы. Итак, в случае, если я говорю здесь, не имеет смысла, придерживайтесь меня для следующих двух видеороликов и, надеюсь, там, используя эти примеры, это объяснение будет иметь немного больше смысла. Но только точка О. Вы можете иметь нейронные сети с другими типами диаграмм, а также то, как связаны нейронные сети, это называется архитектурой. Таким образом, термин архитектура относится к тому, как разные нейроны связаны друг с другом. Это пример другой архитектуры нейронной сети\n",
    "10:07\n",
    "и еще раз вы сможете получить эту интуицию о том, как второй слой, здесь мы имеем три единицы заголовка, которые вычисляют некоторую сложную функцию, возможно, из входного уровня, а затем третий уровень может использовать функции второго уровня и вычислять еще больше сложные функции в третьем слое, так что к тому моменту, когда вы дойдете до выходного слоя, четвертого уровня, у вас могут быть еще более сложные функции того, что вы можете вычислить в третьем слое, и поэтому получите очень интересные нелинейные гипотезы.\n",
    "10:36\n",
    "Кстати, в такой сети, как первый, это называется входным слоем. Четвертый уровень по-прежнему является нашим выходным слоем, и эта сеть имеет два скрытых слоя. Итак, все, что не является уровнем ввода или выходным слоем, называется скрытым слоем.\n",
    "10:53\n",
    "Итак, надеюсь, из этого видео вы получили представление о том, как шаг распространения прямой подачи в нейронной сети работает там, где вы начинаете с активирования входного слоя, и вперед распространяете это на первый скрытый слой, затем на второй скрытый слой, а затем, наконец, выходной слой. И вы также видели, как мы можем векторизовать это вычисление.\n",
    "11:13\n",
    "В следующем я понял, что некоторые из интуиций в этом видео о том, как, как вы знаете, другие определенные слои, вычисляют сложные функции ранних слоев. Я понял, что часть этой интуиции может быть еще немного абстрактной и довольно высокого уровня. Итак, то, что я хотел бы сделать в двух видео, - это подробный пример того, как нейронная сеть может использоваться для вычисления нелинейных функций ввода и надеемся, что это даст вам хорошее представление о сложных нелинейных гипотезах, которые мы имеем может выйти из нейронных сетей.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0:00\n",
    "В предыдущих видео мы говорили об алгоритме градиентного спуска, о модели линейной регрессии и о функции затрат, определенной через сумму среднеквадратических отклонений. Теперь мы сведем вместе градиентный \n",
    "0:20\n",
    "спуск и нашу функции стоимости, что даст нам алгоритм линейной регрессии для аппроксимации данных прямой линией. Напомню, к чему мы пришли в предыдущих видео. Это хорошо знакомый нам алгоритм градиентного спуска, а это наша модель линейной регрессии: линейная гипотеза и усредненная сумма квадратов отклонений, наша функция затрат. Я собираюсь применить \n",
    "1:13\n",
    "градиентный спуск к нашей функции стоимости. Чтобы применить алгоритм и написать \n",
    "1:27\n",
    "программу, в первую очередь нам нужно получить эту производную. Давайте посчитаем эту частную производную... подставим функцию J... то есть коэффициент... сумма от 1 до m квадрата ошибки... Пока я просто переписал сюда определение функции затрат, упростим еще немного... сумма от 1 до m... тета нулевое плюс тета первое на x(i) минус y(i) и все это в квадрате. Теперь я просто подставил формулу функции-гипотезы. И, собственно говоря, \n",
    "3:14\n",
    "нам нужно получить эти частные производные для двух случаев: для j, \n",
    "3:23\n",
    "равного 0, и для j, равного 1. То есть взять ее относительно тета нулевого и тета \n",
    "3:39\n",
    "первого. \n",
    "3:43\n",
    "Я просто напишу, чему они равны. \n",
    "3:47\n",
    "В первом случае получится 1/m умножить на сумму по \n",
    "3:52\n",
    "обучающему набору... h от x(i) минус y(i). А во втором частная производная по тета первому получится равна... то же самое, умножить на x(i). Отлично. \n",
    "4:24\n",
    "Расчет этих частных производных, то есть получение этих выражений из этого, требует представления об анализе функций многих переменных. Если вы знакомы с многомерным анализом, можете сами провести выкладки и убедиться, что частные производные действительно получатся такими, как у меня. А если не знакомы, ничего страшного, \n",
    "5:03\n",
    "можете просто использовать выведенные мной выражения. В домашнем задании анализ вам тоже не понадобится, для реализации градиентного спуска достаточно готовых производных. Итак, получив эти выражения, эти производные, \n",
    "5:38\n",
    "соответствующие уклону графика функции стоимости J, мы можем подставить их в формулы алгоритма градиентного спуска. Вот формулы шага градиентного спуска для линейной регрессии, которые мы будем применять до схождения. Новое значение для тета нулевого и тета первого получаем, вычитая из старого производную, умноженную на альфа. Вот она. Итак, это алгоритм линейной регрессии. \n",
    "6:41\n",
    "Верно? равен, соответственно, \n",
    "6:47\n",
    "частной производной по тета нулевому, которую мы получили на предыдущем слайде. \n",
    "6:57\n",
    "А во втором — частной производной по тета первому, \n",
    "7:08\n",
    "тоже полученной на предыдущем слайде. \n",
    "7:21\n",
    "На всякий случай напомню одну тонкость реализации градиентного спуска: обновлять тета нулевое и тета первое вам нужно одновременно. Посмотрим, как градиентный спуск работает. Если помните, у градиентного спуска была одна проблема: он может «застрять» в локальном экстремуме. Когда я показывал вам градиентный спуск, я пользовался этим графиком, по которому мы спускались, как с холма, и оказалось, что мы можем прийти в разные локальные экстремумы в зависимости от того, откуда начали. Вы можете прийти сюда или сюда. Но, как выясняется, функция стоимости для линейной регрессии всегда будет чашеобразной, как на этом графике. Математический термин для этого — выпуклая функция. Я не буду давать строгого определения выпуклой функции, говоря простым языком, выпуклая функция и функция с чашеобразным графиком, ну, условно \n",
    "9:05\n",
    "чашеобразным, — это одно и то же. У такой функции нет никаких локальных экстремумов, кроме одного глобального. Таким образом, применив градиентный спуск к выпуклой функции, а при линейной регрессии она всегда выпуклая, вы всегда окажетесь в глобальном экстремуме, потому что других локальных экстремумов нет. Теперь посмотрим на алгоритм в действии. Как обычно, здесь у меня графики функции-гипотезы и функции стоимости J. Пусть начальные значения моих параметров соответствуют этой точке. \n",
    "9:55\n",
    "Обычно мы задаем в качестве начальных значений нули. Но для иллюстрации этого случая я положил тета нулевое равным примерно 900, а тета первое — примерно −0,1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
